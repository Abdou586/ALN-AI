import streamlit as st
from langchain_community.chat_models import ChatOllama
from langchain.memory import ConversationBufferMemory
import os
from dotenv import load_dotenv
import json
import datetime
from fpdf import FPDF
import base64
from langchain.schema import HumanMessage, AIMessage
from langchain.chains import ConversationChain

# Charger les variables d'environnement
load_dotenv()

# Configuration de la page Streamlit
st.set_page_config(
    page_title="ALN AI",
    page_icon="ü§ñ",
    layout="wide"
)

# Style CSS personnalis√©
st.markdown("""
    <style>
    .stTextInput>div>div>input {
        background-color: #f0f2f6;
        color: #1f2937;
    }
    .stTextArea>div>div>textarea {
        background-color: #f0f2f6;
        color: #1f2937;
    }
    .chat-message {
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .chat-message.user {
        background-color: #2b313e;
        color: white;
    }
    .chat-message.assistant {
        background-color: #f0f2f6;
    }
    @keyframes popup {
        0% { transform: translate(-50%, -50%) scale(0.8); opacity: 0; }
        50% { transform: translate(-50%, -50%) scale(1.1); }
        100% { transform: translate(-50%, -50%) scale(1); opacity: 1; }
    }
    @keyframes fadeOut {
        0% { transform: translate(-50%, -50%) scale(1); opacity: 1; }
        100% { transform: translate(-50%, -50%) scale(0.8); opacity: 0; }
    }
    .popup-welcome {
        position: fixed;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        width: 90%;
        max-width: 300px;
        padding: 2rem;
        text-align: center;
        animation: popup 0.8s ease-out, fadeOut 0.8s ease-out 3s forwards;
        z-index: 1000;
    }
    .popup-text {
        font-size: 1.5rem;
        color: white;
        margin: 0;
        word-wrap: break-word;
        text-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
    }
    @media (max-width: 480px) {
        .popup-welcome {
            width: 85%;
            padding: 1.5rem;
        }
        .popup-text {
            font-size: 1.3rem;
        }
    }
    </style>
""", unsafe_allow_html=True)

# Initialisation de la session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(return_messages=True)
if "learning_data" not in st.session_state:
    st.session_state.learning_data = {
        "interactions": [],
        "preferences": {},
        "knowledge": {}
    }
if "first_message" not in st.session_state:
    st.session_state.first_message = True

# Titre de l'application
if st.session_state.first_message:
    st.title("ü§ñ ALN AI")
    
    # Message de bienvenue en popup
    st.markdown("""
        <div class="popup-welcome">
            <p class="popup-text">üëã Bienvenue sur ALN AI</p>
        </div>
    """, unsafe_allow_html=True)

# Initialisation du mod√®le LLM
def load_model():
    try:
        # Utiliser un mod√®le Ollama qui g√®re bien les instructions, comme Mistral
        llm = ChatOllama(model="mistral", temperature=0.7)
        return llm
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le Ollama: {str(e)}")
        st.info("Veuillez v√©rifier qu'Ollama est install√© et que le mod√®le 'mistral' est disponible (ollama pull mistral).")
        return None

# Fonction pour sauvegarder les donn√©es d'apprentissage
def save_learning_data():
    try:
        with open("learning_data.json", "w", encoding="utf-8") as f:
            json.dump(st.session_state.learning_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"Erreur lors de la sauvegarde des donn√©es d'apprentissage: {str(e)}")

# Fonction pour charger les donn√©es d'apprentissage
def load_learning_data():
    try:
        if os.path.exists("learning_data.json"):
            with open("learning_data.json", "r", encoding="utf-8") as f:
                st.session_state.learning_data = json.load(f)
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es d'apprentissage: {str(e)}")

# Liste d'abr√©viations et de variantes de salutations
SALUTATIONS = [
    "salut", "slt", "bonjour", "bjr", "cc", "coucou", "hello", "hi", "hey", "yo", "wesh", "re", "bonsoir"
]

# Fonction pour d√©tecter si le message est une salutation seule
def is_salutation_only(prompt: str) -> bool:
    prompt_clean = prompt.lower().strip()
    return any(prompt_clean == s for s in SALUTATIONS)

# Fonction pour g√©n√©rer un PDF
def generate_pdf(text: str) -> tuple:
    try:
        pdf = FPDF()
        pdf.add_page()
        
        # Titre principal
        pdf.set_font("Arial", "B", 16)
        pdf.cell(0, 10, 'Document G√©n√©r√© par ALN AI', ln=True, align='C')
        pdf.ln(10)
        
        # Date
        pdf.set_font("Arial", "I", 10)
        pdf.cell(0, 10, f'G√©n√©r√© le {datetime.datetime.now().strftime("%d/%m/%Y √† %H:%M")}', ln=True, align='R')
        pdf.ln(10)
        
        # Contenu
        pdf.set_font("Arial", "", 12)
        pdf.multi_cell(0, 10, text)
        
        # Pied de page
        pdf.set_y(-15)
        pdf.set_font("Arial", "I", 8)
        pdf.cell(0, 10, 'D√©velopp√© par Abdou Latif Niabaly', 0, 0, 'C')
        
        # Sauvegarder le PDF
        pdf_path = "output.pdf"
        pdf.output(pdf_path)
        
        # Lire le fichier PDF et le convertir en base64
        with open(pdf_path, "rb") as f:
            pdf_bytes = f.read()
            b64_pdf = base64.b64encode(pdf_bytes).decode()
        
        return pdf_path, b64_pdf
    except Exception as e:
        st.error(f"Erreur lors de la g√©n√©ration du PDF: {str(e)}")
        return None, None

# Fonction pour la conversation (mise √† jour pour utiliser l'historique)
def advanced_chat(prompt: str) -> str:
    try:
        llm = load_model()
        if llm is None:
            return "Erreur: Mod√®le non charg√©"

        # Construire l'historique de chat pour la cha√Æne
        # Ancienne ligne retir√©e : chat_history = st.session_state.memory.load_memory_variables({})['chat_history']

        # Cr√©er la cha√Æne de conversation
        # Passer le prompt syst√®me au d√©but de la conversation
        conversation_chain = ConversationChain(
            llm=llm,
            memory=st.session_state.memory,
            verbose=False # Mettre √† True pour d√©bugger les prompts envoy√©s au mod√®le
        )
        
        # On peut ajouter une logique pour les salutations et l'identit√© AVANT d'appeler la cha√Æne
        # pour garantir une r√©ponse rapide et sp√©cifique pour ces cas.

        # V√©rifier si c'est une question sur l'identit√©
        if any(phrase in prompt.lower() for phrase in ["qui es-tu", "ton nom", "qui t'a cr√©√©", "qui t'a fait", "qui t'a d√©velopp√©"]):
            # Ajouter la question utilisateur √† l'historique pour maintenir le contexte
            st.session_state.memory.chat_memory.add_user_message(prompt)
            response = "Je suis ALN AI, d√©velopp√© par Abdou Latif Niabaly. Je suis con√ßu pour vous aider de mani√®re pr√©cise et efficace." # R√©ponse directe
            st.session_state.memory.chat_memory.add_ai_message(response)
            return response
        
        # Si c'est une salutation seule
        if is_salutation_only(prompt):
             # Ajouter la question utilisateur √† l'historique
            st.session_state.memory.chat_memory.add_user_message(prompt)
            response = "Bonjour ! Je suis ALN AI, d√©velopp√© par Abdou Latif Niabaly. Comment puis-je vous aider ?" # R√©ponse personnalis√©e
            st.session_state.memory.chat_memory.add_ai_message(response)
            return response
        
        # Ajouter une logique pour les messages d'acquittement simples
        ACKNOWLEDGMENT_PHRASES = ["ok", "d'accord", "merci", "merci beaucoup", "cool", "compris"]
        if prompt.lower().strip() in ACKNOWLEDGMENT_PHRASES:
            # Ajouter la question utilisateur √† l'historique
            st.session_state.memory.chat_memory.add_user_message(prompt)
            response = "De rien !" # R√©ponse minimale
            st.session_state.memory.chat_memory.add_ai_message(response)
            return response

        # D√©tecter les requ√™tes qui n√©cessitent une liste ou une s√©rie d'√©l√©ments
        list_request_keywords = ["donne", "liste", "√©num√®re", "cite", "propose", "commandes", "film"] # Ajoutez d'autres mots-cl√©s si n√©cessaire
        is_list_request = any(keyword in prompt.lower() for keyword in list_request_keywords)

        # Si la question est vide ou trop courte (peut √™tre g√©r√© par Streamlit input validation)
        if len(prompt.strip()) < 2:
            response = "Pouvez-vous pr√©ciser votre demande ?"
            # Ajouter √† l'historique si on veut que cette interaction soit m√©moris√©e
            # st.session_state.memory.chat_memory.add_user_message(prompt)
            # st.session_state.memory.chat_memory.add_ai_message(response)
            return response # Pas besoin d'appeler le mod√®le pour √ßa

        # Si ce n'est pas un cas sp√©cial, utiliser la cha√Æne de conversation
        # Inclure le prompt syst√®me dans le message utilisateur ou dans le template de prompt de la cha√Æne
        # L'approche la plus simple avec ConversationChain est de laisser le prompt syst√®me global
        # et de s'assurer qu'il est bien compris par le mod√®le.

        # Pour s'assurer que les r√®gles du prompt syst√®me sont toujours pr√©sentes,
        # on peut les inclure AVANT le message de l'utilisateur dans l'input de la cha√Æne.
        # Cependant, ConversationChain a son propre template. Modifions le prompt syst√®me global
        # pour qu'il soit plus adapt√© √† √™tre combin√© avec l'historique par la cha√Æne.
        
        # Revenir √† un prompt syst√®me qui d√©finit le r√¥le et les r√®gles g√©n√©rales,
        # la cha√Æne g√©rera l'historique.
        system_instructions = """
Tu es ALN AI. R√©ponds EN FRAN√áAIS.

R√®gles de r√©ponse EXIGENTES :
- **Langue :** R√©ponds TOUJOURS et UNIQUEMENT en Fran√ßais.
- **Sujet :** Concentre-toi UNIQUEMENT sur la question actuelle de l'utilisateur. IGNORE tout contexte de conversation pr√©c√©dent qui n'est pas pertinent pour la nouvelle question.
- **Concision & D√©tail :** Sois concis pour les questions simples. Fournis des d√©tails SEULEMENT si n√©cessaire ou demand√©. Pour les listes ou informations sp√©cifiques, donne EXACTEMENT ce qui est demand√© (nombre d'√©l√©ments, d√©tails sp√©cifiques).
- **D√©but de r√©ponse STRICT :** Commence IMM√âDIATEMENT ta r√©ponse par l'information ou la liste demand√©e. Ne commence JAMAIS par une salutation, une auto-pr√©sentation, une r√©f√©rence √† des sujets pr√©c√©dents, ou une phrase d'introduction g√©n√©rique comme "Voici..." ou "Commande git utilise...".
- **√âviter :** N'utilise PAS de phrases g√©n√©riques. N'invente PAS d'informations.
- **Qualit√© des listes/informations :** Si une liste ou des informations sp√©cifiques sont demand√©es (ex: 'commandes git utiles'), assure-toi qu'elles sont FACTUELLEMENT correctes, pertinentes, utiles et format√©es clairement (par exemple, avec des puces ou des num√©ros pour les listes).
"""

        # Ajuster le prompt envoy√© √† la cha√Æne si c'est une requ√™te de liste
        if is_list_request:
            # Ajouter une instruction explicite pour g√©n√©rer une liste
            full_prompt_with_instructions = system_instructions + "\n\nR√©ponds √† la demande suivante en fournissant DIRECTEMENT la liste ou les informations demand√©es, sans introduction :\n" + prompt
        else:
            full_prompt_with_instructions = system_instructions + "\n\n" + prompt

        # Utiliser l'historique g√©r√© par la m√©moire dans la cha√Æne
        # La m√©thode predict de ConversationChain prend l'input utilisateur
        response_object = conversation_chain.invoke(input=full_prompt_with_instructions)
        
        # La r√©ponse de conversation_chain.invoke() est un dictionnaire, pas un objet AIMessage direct.
        # L'output textuel est g√©n√©ralement sous la cl√© 'response' ou similaire selon la cha√Æne.
        # Pour ConversationChain, c'est sous la cl√© 'response'.

        response_text = response_object.get('response', str(response_object))
        response_text = response_text.strip()
        
        # Nettoyage am√©lior√© : supprimer toute salutation, auto-pr√©sentation ou phrase g√©n√©rique en d√©but de r√©ponse
        phrases_a_supprimer_debut = [
            "Oui, je suis le syst√®me ALN AI. Je r√©pondrai √† ta question en fran√ßais.",
            "Bonjour ! Je suis ALN AI, d√©velopp√© par Abdou Latif Niabaly. Comment puis-je vous aider ?", # D√©j√† g√©r√© par logique sp√©cifique mais s√©curit√©
            "De rien !", # D√©j√† g√©r√© par logique sp√©cifique mais s√©curit√©
            "Pouvez-vous pr√©ciser votre demande ?", # D√©j√† g√©r√© par logique sp√©cifique mais s√©curit√©
            "Bonjour !", "Salut !", "Hello !", "Hi !",
            "Je suis un assistant", "Je suis une IA", "Je suis cr√©√© par", "Je suis d√©velopp√© par",
            "Je suis d√©sol√©", "En tant qu'assistant",
            "Pour r√©pondre √† ta demande, je te donnerai", # Phrase d'introduction d√©tect√©e
            "Commande git utilise pour le contr√¥le de version Git :" # Introduction sp√©cifique √† la liste git
            # Ajoutez d'autres phrases d√©tect√©es si n√©cessaire
        ]
        
        for phrase in phrases_a_supprimer_debut:
             if response_text.startswith(phrase):
                 # Utiliser rstrip() pour √©viter de laisser un espace apr√®s la suppression si la phrase ne finit pas par un point/espace
                 response_text = response_text[len(phrase):].lstrip()
                 # On peut casser la boucle car on suppose qu'il n'y a qu'une seule phrase d'intro au d√©but
                 break
        
        # La m√©moire est mise √† jour automatiquement par la ConversationChain
        
        return response_text

    except Exception as e:
        # Cette partie peut aussi b√©n√©ficier de l'ajout de l'erreur √† l'historique si on veut la m√©moriser
        error_message = f"Erreur lors de la conversation : {str(e)}"
        # st.session_state.memory.chat_memory.add_ai_message(error_message) # Optionnel : m√©moriser les erreurs
        return error_message

# Charger les donn√©es d'apprentissage au d√©marrage
load_learning_data()

# Sidebar avec des informations
with st.sidebar:
    st.header("√Ä propos")
    st.markdown("""
    Cette application utilise:
    - Streamlit pour l'interface utilisateur
    - HuggingFace pour l'IA
    - Syst√®me d'apprentissage autonome
    """)
    
    st.markdown("""
    <div style='margin-top: 20px;'>
        <a href='https://www.linkedin.com/in/abdou-latif-niabaly-10bb45268/' target='_blank' style='text-decoration: none; color: #0077B5; margin-right: 20px;'>
            <span style='font-size: 16px;'>üë®‚Äçüíª LinkedIn</span>
        </a>
        <a href='https://github.com/Abdou586' target='_blank' style='text-decoration: none; color: #0077B5;'>
            <span style='font-size: 16px;'>üíª GitHub</span>
        </a>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Initialiser l'√©tat du formulaire s'il n'existe pas
    if 'show_contact_form' not in st.session_state:
        st.session_state.show_contact_form = False
    
    # Bouton toggle pour le formulaire
    if st.button("üìß Me contacter", use_container_width=True):
        st.session_state.show_contact_form = not st.session_state.show_contact_form
        st.rerun()
    
    # Afficher le formulaire si show_contact_form est True
    if st.session_state.show_contact_form:
        with st.form("contact_form"):
            name = st.text_input("Votre nom")
            email = st.text_input("Votre email")
            subject = st.text_input("Sujet")
            message = st.text_area("Message")
            submit_button = st.form_submit_button("Envoyer")
            
            if submit_button:
                if name and email and subject and message:
                    # Pr√©paration du message
                    email_message = f"""
                    Nouveau message de contact re√ßu :
                    
                    Nom: {name}
                    Email: {email}
                    Sujet: {subject}
                    
                    Message:
                    {message}
                    """
                    
                    # Envoi du message (√† impl√©menter avec votre service d'email pr√©f√©r√©)
                    st.success("Message envoy√© avec succ√®s ! Je vous r√©pondrai d√®s que possible.")
                    st.session_state.show_contact_form = False
                    st.rerun()
                else:
                    st.error("Veuillez remplir tous les champs du formulaire.")

# Affichage des messages pr√©c√©dents
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Zone de saisie pour l'utilisateur
if prompt := st.chat_input("Posez votre question ici..."):
    # Mettre √† jour l'√©tat du premier message
    st.session_state.first_message = False
    
    # Ajouter le message de l'utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Obtenir la r√©ponse de l'assistant
    with st.chat_message("assistant"):
        with st.spinner("ALN AI r√©fl√©chit..."):
            try:
                response = advanced_chat(prompt)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                # Si l'utilisateur demande un PDF, g√©n√©rer le PDF et proposer le t√©l√©chargement
                if "pdf" in prompt.lower():
                    pdf_path, b64_pdf = generate_pdf(response)
                    if pdf_path and b64_pdf:
                        st.success("PDF g√©n√©r√© avec succ√®s !")
                        # Cr√©er un lien de t√©l√©chargement
                        href = f'<a href="data:application/pdf;base64,{b64_pdf}" download="document.pdf">Cliquez ici pour t√©l√©charger le PDF</a>'
                        st.markdown(href, unsafe_allow_html=True)
            except Exception as e:
                error_message = f"Une erreur s'est produite : {str(e)}"
                st.error(error_message)
                st.error("Veuillez r√©essayer avec une autre question.")
                st.session_state.messages.append({"role": "assistant", "content": error_message}) 